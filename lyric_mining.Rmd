---
title: "Mining Lyrics"
output: html_notebook
---

```{r}
library(tidyverse)
library(magrittr)
library(genius)
library(kernlab) 
library(caret) 
library(tm) 
library(splitstackshape)
library(e1071)
library(httr)		# Library for scraping web data
library(jsonlite)	# Library for reading JSON data
```
```{r Functions for scraping data}
# Retrieving an artist's MusicBrainz ID from last.fm
get_artist_mbid <- function(name_artist){

  url <- "http://ws.audioscrobbler.com/2.0/?"
  method <- "method=artist.getInfo"
  parameter <- paste0("artist=", name_artist)
  api_key <- "api_key=bd357ecfcaa0085f4948c273729f44f4"
  format <- "format=json"
  qty_albums <- "limit=1"
  rest_request <- paste0(url, paste(method, parameter, api_key, format, sep = "&"))
  
  json <- GET( rest_request ) #, use_proxy("172.16.16.62", 8080))	# Executing request
  json_text <- content(json, as="text", encoding = "UTF-8")				# Get JSON text data
  ls_artist <- fromJSON(json_text)				# loading text data in a list

  return(ls_artist[1]$artist$mbid)
}
``` 
```{r}
# Retrieving an artist's studio albums from MusicBrainz
get_artist_studio_albums <- function(artist_mbid){
  
  url <- "http://musicbrainz.org/ws/2/artist/"
  filter <- "?inc=release-groups"
  format <- "fmt=json"
  rest_request <- paste0(url, artist_mbid, filter, "&", format)
  
  # The while loop is done because the MusicBrainz JSON API is unstable
  ls_albums <- NULL
  attempt <- 0

  while( (length(ls_albums) == 0 | is.null(ls_albums)) && attempt <= 99 ) {
    
    ls_albums <- NULL
    attempt <- attempt + 1
    json <- GET(rest_request)    # , use_proxy("172.16.16.62", 8080))	# Executing request
    json_text <- content(json, as="text", encoding = "UTF-8")				# Get JSON text data
    try(
      ls_albums <- fromJSON(json_text)  # loading text data in a list
    )
  }
  
  if(!is.null(ls_albums) & length(ls_albums) > 0 & is.null(ls_albums[["error"]])){
    
    ls_album_types <- ls_albums$`release-groups`$`secondary-types`
    if(sum(unlist(lapply(ls_album_types, length))) > 0){
      ls_album_types <- lapply(ls_album_types, function(x) ifelse(is.null(x), NA, x))
    } else {
      ls_album_types <- rep(NA, nrow(ls_albums$`release-groups`))
    }

    df <- tibble(artist = rep(ls_albums$`sort-name`, nrow(ls_albums$`release-groups`)),
                 album = ls_albums$`release-groups`$title,
                 date_appearance = ls_albums$`release-groups`$`first-release-date`,
                 album_type = unlist(ls_album_types))
    df %<>%
      filter(is.na(album_type)) %>% 
      mutate(year_release = str_sub(date_appearance, 1, 4)) %>% 
      select(artist, album, year_release)
    row.names(df_albums) <- NULL    
  } else {
    
    df <- NULL
  }

  return(df)
}
``` 
```{r}
# Retrieving an artist's studio albums from last.fm
get_artist_albums <- function(name_artist){

  url <- "http://ws.audioscrobbler.com/2.0/?"
  method <- "method=artist.getTopAlbums"
  parameter <- paste0("artist=", name_artist)
  api_key <- "api_key=bd357ecfcaa0085f4948c273729f44f4"
  format <- "format=json"
  qty_albums <- "limit=150"
  rest_request <- paste0(url, paste(method, parameter, api_key, format, delimiter = "&"))
  
  # Executing REST request and getting josn text data
  json <- GET( rest_request ) #, use_proxy("172.16.16.62", 8080))	# Executing request
  json_text <- content(json, as = "text")				# Get JSON text data
  ls_albums <- fromJSON(json_text)				# loading text data in a list

  df_albums <- ls_albums$topalbums$album
  df_albums <- as.tibble(cbind(artist = rep(name_artist, nrow(df_albums)),
                               album = df_albums$name))
  return(df_albums)
}
```

# Determining artists
```{r}
vec_artists <- c("Tindersticks", "Tom Waits", "Nick Cave and The Bad Seeds", "Vanilla Ice", "Metallica", "Miley Cyrus", 
                 "Eminem", "Busta Rhymes", "The Beatles", "Bruce Springsteen", "ABBA", "Justin Timberlake", "Beyoncé", 
                 "Destiny's Child", "Nina Simone")
df_artists <- tibble(artist = vec_artists,
                     artist_mbid = sapply(vec_artists, get_artist_mbid))
rm(vec_artists)
```

# Retrieving albums

## Pre-loading albums if there are any
```{r, message=TRUE}
file_albums <- "artists_albums.csv"

if(file.exists(file_albums)){
  df_albums <- read_csv2(file_albums)
  df_albums %<>% filter(artist %in% df_artists$artist)
} else {
  df_albums <- tibble(artist = character(0),
                      album = character(0),
                      year_release = character(0))
}
rm(file_albums)

artists_search <- df_artists$artist_mbid[!df_artists$artist %in% df_albums$artist]
```

## Getting albums
```{r}
for(artist_mbid in artists_search){

  df_artist_albums <- tibble(artist = character(0), album = character(0), year_release = character(0))
  
  df_artist_albums <- get_artist_studio_albums(artist_mbid)

  if(nrow(df_artist_albums) > 0) {
    # Rename artist back to original form
    name_artist <- df_artists$artist[df_artists$artist_mbid == artist_mbid]
    df_artist_albums$artist = name_artist
  
    df_albums <- rbind(df_albums, df_artist_albums)  
  
    print(paste0("Added ", name_artist, " - ", nrow(df_artist_albums), " albums"))
    rm(name_artist)
  }
  rm(df_artist_albums)    
}

rm(artist_mbid, artists_search)

write_csv2(df_albums, "artists_albums.csv")
```

# Getting lyrics
```{r Loading data}
# Read albums and pre-processed lyrics
file_lyrics <- "df_lyrics.RDS"
if(file.exists(file_lyrics)){
  df_lyrics <- read_rds(file_lyrics) 
} else {
  df_lyrics <- tibble(artist = character(0), album = character(0))
}

# See whether new albumn lyrics should be retrieved
df_lyrics_new <- df_albums %>% 
  anti_join(df_lyrics, by = c("artist", "album")) %>% 
  add_genius(artist, album)

# Add new album lyrics
if(nrow(df_lyrics) != 0){
  df_lyrics <- rbind(df_lyrics, df_lyrics_new)
} else {
  df_lyrics <- df_lyrics_new
}

# Write found lyrics to intermediary file
if(nrow(df_lyrics_new) > 0) {
  
  # Deduplicate songs
  df_lyrics %<>% 
    mutate(year_release = ifelse(is.na(year_release), "9999", year_release)) %>% 
    group_by(artist, track_title) %>% 
    mutate(year_release_first = min(year_release)) %>% 
    ungroup() %>% 
    filter(year_release_first == year_release) %>% 
    select(-year_release_first) %>% 
    group_by(artist, track_title) %>% 
    mutate(track_n_first = min(track_n)) %>% 
    ungroup() %>% 
    filter(track_n_first == track_n) %>% 
    select(-track_n_first) 

  write_rds(df_lyrics, file_lyrics, compress = "gz")
}
  
rm(file_lyrics, df_lyrics_new)

# Albums without lyrics
df_albums %>% 
  anti_join(df_lyrics, by = c("artist", "album")) 
```

# Making lyrics per song
```{r}
# Create row per track, combining lyrics in one text field
df_lyric_dox <- df_lyrics %>% 
  filter(!is.na(lyric)) %>% 
  group_by(artist, album, year_release, track_title, track_n) %>% 
  summarise(lyric = paste0(lyric, collapse = " ")) %>% 
  ungroup() %>% 
  mutate(doc_id = row_number()) %>% 
  dplyr::select(doc_id, text = lyric, everything())

# Make factor of artists for modelling
levels_artists <- unique(df_lyric_dox$artist)
df_lyric_dox$artist <- factor(df_lyric_dox$artist, levels = levels_artists)
```

# Method 1: text2vec

Why use text2vec? Texts themselves can take up a lot of memory, but vectorized texts usually do not, because they are stored as sparse matrices. Because of R’s copy-on-modify semantics, it is not easy to iteratively grow a DTM. Thus constructing a DTM, even for a small collections of documents, can be a serious bottleneck for analysts and researchers. It involves reading the whole collection of text documents into RAM and processing it as single vector, which can easily increase memory use by a factor of 2 to 4. The text2vec package solves this problem by providing a better way of constructing a document-term matrix.

```{r}
library(text2vec)
library(data.table)

# Setting up training and testing set
set.seed(42)
all_ids <- df_lyric_dox$doc_id
qty_train <- round(length(all_ids) * .8, 0)
train_ids <- sample(all_ids, qty_train)
test_ids <- setdiff(all_ids, train_ids)
df_train <- df_lyric_dox[train_ids, ]
df_test <- df_lyric_dox[test_ids, ]
rm(qty_train, all_ids)
```

_itoken()_ : Creates an iterator over tokens. All functions prefixed with create_ from the __text2vec__ library work with these iterators. The iterator abstraction allows us to hide most of details about input and to process data in memory-friendly chunks.
Faster and more extensive tokenizers from the library [tokenizers](https://cran.r-project.org/web/packages/tokenizers/vignettes/introduction-to-tokenizers.html) can be used.

_create_vocabulary()_: Creates the vocabulary. In the text mining field the concept 'terms' are used instead of words, because the unit of analysis could be single words, two word units (bi-grams) or up to any length phrases (n-grams). By default it creates terms of of word in lenght, but this can be set to different ranges using the _ngram_ argument. For example by setting it to `ngram = c(1L, 2L)` the term length wil be set to 1 and 2 word lengths.

_create_dtm()_: Creates a Document Term Matric (DTM), which shows for each document (lyrics as rows), the frequency of word usage (each term is a column).

_prune_vocabulary()_: Removes uncommon term, terms which are in few documents and words which are overrepresented in documents. This would remove statisticly irrelevant terms 

```{r}
prep_data_set <- function(df_dataset){
  
  # define preprocessing function and tokenization function
  iterator_terms <- itoken(df_dataset$text,
                           preprocessor = tolower, 
                           tokenizer = word_tokenizer, 
                           ids = df_dataset$doc_id, 
                           progressbar = FALSE)

  vocab <- create_vocabulary(iterator_terms,
                             stopwords = c(stopwords("english"), "instrumental"))

  # Prune vocabulary: removing uncommon words in few documents and words which are overrepresented in documents
  vocab <- prune_vocabulary(vocab,
                            term_count_min = 2,
                            doc_proportion_max = 0.5,
                            doc_proportion_min = 0.001)
   
  # Transform tokens into vector space - i.e. how to map words to indices
  vectorizer = vocab_vectorizer(vocab)
  
  # create document-term matrix
  dtm_dataset = create_dtm(iterator_terms, vectorizer)

  df_dtm_dataset <- as_tibble(as.matrix(dtm_dataset))
  df_dtm_dataset$artist <- df_dataset$artist

  lst_return <- list(dtm_dataset = dtm_dataset,
                     tdf_dtm_dataset = df_dtm_dataset,
                     vocab = vocab)

  return(lst_return)
}
```
```{r}
lst_lyrics <- prep_data_set(df_lyric_dox)
lst_train <- prep_data_set(df_train)
lst_test <- prep_data_set(df_test)
```
```{r}
df_word_freq <- lst_lyrics$df_dtm_dataset %>% 
  gather(key = term, value = qty, -artist) %>% 
  group_by(term) %>% 
  summarise(qty = sum(qty)) %>% 
  ungroup() 

df_word_freq %>% 
  arrange(desc(qty)) %>% 
  head(20) %>% 
ggplot(aes(x=reorder(term, qty), y = qty)) +
  geom_col(fill = "steelblue") +
  scale_y_continuous(expand = c(0,0)) +
  labs(x = "", y = "Word count") +
  theme_bw() +
  coord_flip()
```
```{r, fig.height=6}
library(wordcloud)

wordcloud(df_word_freq$term, df_word_freq$qty, max.words = 75, colors = c("aquamarine","darkgoldenrod","tomato"))
```
```{r, fig.height=7}
df_dtm_lyrics <- lst_lyrics$df_dtm_dataset
df_dtm_lyrics$track_title <- df_lyric_dox$track_title

df_artist_word_freq <-  df_dtm_lyrics %>% 
  gather(key = term, value = freq_term, -artist, -track_title) %>% 
  mutate(qty_songs = 1) %>% 
  group_by(term, artist) %>% 
  summarise(qty_songs = sum(qty_songs),
            freq_term = sum(freq_term)) %>% 
  ungroup() %>% 
  mutate(perc_of_document = freq_term / qty_songs)

df_artist_word_freq %>% 
  arrange(desc(perc_of_document)) %>%
  group_by(artist) %>% 
  mutate(rank = row_number()) %>% 
  ungroup() %>% 
  filter(rank <= 7) %>% 
ggplot(aes(x=reorder(term, perc_of_document), y = perc_of_document)) +
  geom_col(fill = "steelblue") +
  scale_y_continuous(expand = c(0,0)) +
  labs(x = "", y = "Word count") +
  facet_grid(.~artist) +
  theme_bw() +
  coord_flip()
```
# TF_IDF
```{r}
dtm_lyrics <- lst_lyrics$dtm_dataset

# define tfidf model
tfidf = TfIdf$new()
# fit model to train data and transform train data with fitted model
dtm_lyrics_tfidf = fit_transform(dtm_lyrics, tfidf)

df_dtm_lyrics_tfidf <- as_tibble(as.matrix(dtm_lyrics_tfidf))
df_dtm_lyrics_tfidf$name_artist <- df_lyric_dox$artist
df_dtm_lyrics_tfidf$track_title <- df_lyric_dox$track_title
df_dtm_lyrics_tfidf %>% 
  gather(key = term, value = freq_term, -name_artist, -track_title) %>% 
  filter(freq_term > 0) %>% 
  arrange(desc(freq_term))
# tfidf modified by fit_transform() call!
# apply pre-trained tf-idf transformation to test data
dtm_test_tfidf  = create_dtm(it_test, vectorizer) %>% 
  transform(tfidf)
```

tdidf train and test
```{r}
dtm_train <- lst_train$dtm_dataset
tfidf_train <- TfIdf$new()
dtm_train_tfidf <- fit_transform(dtm_train, tfidf_train)
df_dtm_train_tfidf <- as_tibble(as.matrix(dtm_train_tfidf))
df_dtm_train_tfidf$name_artist <- (df_train$artist)

dtm_test <- lst_test$dtm_dataset
tfidf_test <- TfIdf$new()
dtm_test_tfidf <- fit_transform(dtm_test, tfidf_test)
df_dtm_test_tfidf <- as_tibble(as.matrix(dtm_test_tfidf))
df_dtm_test_tfidf$name_artist <- df_test$artist
```

```{r Create linear SVM model parallel}
library(doParallel)
library(tictoc)
cl <- makePSOCKcluster(4)
registerDoParallel(cl)

tic()
fit_svmLinear3 <- train(name_artist ~ ., data = df_dtm_train_tfidf, method = "svmLinear3")
toc()

## When you are done:
stopCluster(cl)
```
```{r}
df_new <- data.frame(df_dtm_test_tfidf[,intersect(colnames(df_dtm_test_tfidf), 
                                                  colnames(df_dtm_train_tfidf))])

pred_artist_name <- predict(fit_svmLinear3, df_new)

tibble(prediction = pred_artist_name,
       name_artist = df_dtm_test_tfidf$name_artist)
```


```{r}
colnames(lst_train$df_dtm_dataset)

df_train <- data.frame(dtm_train[,intersect(colnames(dtm_train), colnames(dtm_test))])
df_test <- data.frame(dtm_test[,intersect(colnames(dtm_test), colnames(dtm_train))])
```

```{r Predict}
df_pred_svmLinear3 <- predict(fit_svmLinear3, lst_test$df_dtm_dataset) #, type = "prob")
```

```{r}
matrix_confusion <- confusionMatrix(df_pred_svmLinear3, df_test$corpus)
matrix_confusion
```

```{r}
df_accuracy <- cbind(df_lyric_dox[idx_test, ], prediction = df_pred)
df_accuracy$accuracy <- if_else(df_accuracy$prediction == df_accuracy$artist, 1, 0)
```
```{r}
library(performanceEstimation)
classificationMetrics(df_accuracy$artist, df_accuracy$prediction)
```


# Method 2: Corpus
```{r}
# Convert df_source to a corpus: df_corpus
df_source <- VectorSource(df_lyric_dox$text)
corpus_lyrics <- Corpus(df_source)

# Clean corpus
corpus_lyrics %<>% 
  tm_map(removeWords, c("-", "—","“", "‘","…", "NA", "character")) %>% 
  tm_map(content_transformer(tolower)) %>% 
  tm_map(content_transformer(removeNumbers)) %>% 
  tm_map(content_transformer(removePunctuation)) %>% 
  tm_map(removeWords, stopwords("english")) %>% 
  tm_map(content_transformer(stripWhitespace))
```
```{r}
# Create document term and term document matrices
dtm_lyrics <- DocumentTermMatrix(corpus_lyrics)
tdm_lyrics <- TermDocumentMatrix(corpus_lyrics)
```
```{r Getting data for word visualisations}
library(tidytext)
df_lyric_words <- tidy(tdm_lyrics) 
df_lyric_words %<>%
  rename(doc_id = document) %>% 
  mutate(doc_id = as.integer(doc_id)) %>% 
  left_join(df_lyric_dox, by = "doc_id") %>% 
  select(-text)
```

```{r}
df_word_freq <- df_lyric_words %>% 
  group_by(term) %>% 
  summarise(qty = sum(count)) %>% 
  ungroup() 

df_word_freq %>% 
  arrange(desc(qty)) %>% 
  head(20) %>% 
ggplot(aes(x=reorder(term, qty), y = qty)) +
  geom_col(fill = "steelblue") +
  scale_y_continuous(expand = c(0,0)) +
  labs(x = "", y = "Word count") +
  theme_bw() +
  coord_flip()
```
```{r, fig.height=6}
library(wordcloud)

wordcloud(df_word_freq$term, df_word_freq$qty, max.words = 70, colors = c("aquamarine","darkgoldenrod","tomato"))
```
```{r}
df_artist_word_freq <- df_lyric_words %>% 
  group_by(term) %>% 
  summarise(qty = sum(count)) %>% 
  ungroup() 
```


```{r Setting up training and test set}
set.seed(42)
qty_train <- round(length(corpus_lyrics) * .8, 0)
idx_train <- sample(1:length(corpus_lyrics), qty_train)
idx_test <- c(1:length(corpus_lyrics))[!c(1:length(corpus_lyrics)) %in% idx_train]

corpus_train <- corpus_lyrics[idx_train]
corpus_test <- corpus_lyrics[idx_test]
```

```{r Creating document term matrices}
dtm_train <- as.matrix(DocumentTermMatrix(corpus_train, control=list(wordLengths=c(1,Inf))))
dtm_test <- as.matrix(DocumentTermMatrix(corpus_test, control=list(wordLengths=c(1,Inf))))
```

```{r Conforming DTMs}
df_train <- data.frame(dtm_train[,intersect(colnames(dtm_train), colnames(dtm_test))])
df_test <- data.frame(dtm_test[,intersect(colnames(dtm_test), colnames(dtm_train))])
```

```{r}
df_train$corpus <- df_lyric_dox$artist[idx_train]
df_test$corpus <- df_lyric_dox$artist[idx_test]
```

```{r Create ksvm model}
fit_ksvm <- ksvm(corpus ~ ., data = df_train, kernel = "rbfdot")
df_pred <- predict(fit_artists, df_test, type = "prob")
```

```{r Create linear SVM model}
fit_svmLinear3 <- train(corpus ~ ., data = df_train, method = "svmLinear3")
```

```{r Predict}
df_pred_svmLinear3 <- predict(fit_svmLinear3, df_test) #, type = "prob")
```

```{r}
matrix_confusion <- confusionMatrix(df_pred_svmLinear3, df_test$corpus)
matrix_confusion
```

```{r}
df_accuracy <- cbind(df_lyric_dox[idx_test, ], prediction = df_pred)
df_accuracy$accuracy <- if_else(df_accuracy$prediction == df_accuracy$artist, 1, 0)
```
```{r}
library(performanceEstimation)
classificationMetrics(df_accuracy$artist, df_accuracy$prediction)
```
